{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning, Module 4\n",
    "## **Ensemble, Bagging and Boosting**\n",
    "All you need to know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/1200/1*8T4HEjzHto_V8PrEFLkd9A.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/max/1200/1*8T4HEjzHto_V8PrEFLkd9A.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging:\n",
    "is a simple ensembling technique in which we build many independent predictors/models/learners and combine them using some model averaging techniques. (e.g. weighted average, majority vote or normal average)\n",
    "We typically take random sub-sample/bootstrap of data for each model, so that all the models are little different from each other. \n",
    "\n",
    "Each observation is chosen with replacement to be used as input for each of the model. So, each model will have different observations based on the bootstrap process. Because this technique takes many uncorrelated learners to make a final model, it reduces error by reducing variance. \n",
    "\n",
    "Example of bagging ensemble is Random Forest models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting:\n",
    "is an ensemble technique in which the predictors are not made independently, but sequentially.\n",
    "\n",
    "This technique employs the logic in which the subsequent predictors learn from the mistakes of the previous predictors. Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most. (So the observations are not chosen based on the bootstrap process, but based on the error).\n",
    "\n",
    "The predictors can be chosen from a range of models like decision trees, regressors, classifiers etc. Because new predictors are learning from mistakes committed by previous predictors, it takes less time/iterations to reach close to actual predictions.But we have to choose the stopping criteria carefully or it could lead to overfitting on training data.\n",
    "\n",
    "Gradient Boosting is an example of boosting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters:\n",
    "\n",
    "`n_estimators`: the number of estimators (number of desicion trees in the random forest) \n",
    "\n",
    "`max_features`: the number of features to consider in each split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Boosting (AdaBoost)\n",
    "Alters the distribution of the training dataset to increase weights on sample observations that are difficult to classify.\n",
    "\n",
    "The final prediction is based on a majority vote of the weak learner's predictions weighted by their individual accuracy.\n",
    "\n",
    "Weak learners applied are decision stumps (i.e., decision trees with a single split or one-level).\n",
    "\n",
    "The observations in the dataset are weighted such that a greater weight on observations that are misclassified (difficult to classify) versus the observations that have been classified correctly by the model. \n",
    "New weak learners are added sequentially to focus training on the difficult observations. In this manner, observations that are difficult to classify are weighted higher to the point that the algorithm identifies a model that accurately classifies these observations.\n",
    "\n",
    "Adaptive boosting changes the sample distribution being used for training.\n",
    "\n",
    "Intuitively, Adaboost is known as a step-wise additive model. \n",
    "Adaboost initially specifies a set of weak learners and the boosting process is to learn how to add the weights of these weak learners to be a strong learner. The weight of each learner is based on whether it predicts each observation accurately or not.\n",
    "\n",
    "\n",
    "If the learner mispredicts an observation, its weight is reduced. This process is repeated until convergence.\n",
    "Predictions are made by using a majority vote of the weak learner's predictions (i.e., using a set of weak learners and seeing which classification outcome is voted for the most) weighted by their individual accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://static.packt-cdn.com/products/9781788295758/graphics/image_04_046-1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://static.packt-cdn.com/products/9781788295758/graphics/image_04_046-1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "`base_estimators`: specifies the base type estimator, i.e. the algorithm to be used as base learner.\n",
    "\n",
    "`n_estimators`: It defines the number of base estimators, where the default is 10 but you can increase it in order to obtain a better performance.\n",
    "\n",
    "`learning_rate` : same impact as in gradient descent algorithm\n",
    "\n",
    "`max_depth` : Maximum depth of the individual estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "\n",
    "- Step 0: Initialize the weights of data points. if the training set has 100 data points, then each pointâ€™s initial weight should be 1/100 = 0.01.\n",
    "\n",
    "- Step 1: Train a decision tree\n",
    "\n",
    "- Step 2: Calculate the weighted error rate (e) of the decision tree. The weighted error rate (e) is just how many wrong predictions out of total and you treat the wrong predictions differently based on its data pointâ€™s weight. The higher the weight, the more the corresponding error will be weighted during the calculation of the (e).\n",
    "\n",
    "- Step 3: Calculate this decision treeâ€™s weight in the ensemble\n",
    "the weight of this tree = learning rate * log( (1 â€” e) / e)\n",
    "the higher weighted error rate of a tree, ðŸ˜«, the less decision power the tree will be given during the later voting\n",
    "the lower weighted error rate of a tree, ðŸ˜ƒ, the higher decision power the tree will be given during the later voting\n",
    "\n",
    "- Step 4: Update weights of wrongly classified points\n",
    "the weight of each data point =\n",
    "if the model got this data point correct, the weight stays the same\n",
    "if the model got this data point wrong, the new weight of this point = old weight * np.exp(weight of this tree)\n",
    "Note: The higher the weight of the tree (more accurate this tree performs), the more boost (importance) the misclassified data point by this tree will get. The weights of the data points are normalized after all the misclassified points are updated.\n",
    "\n",
    "- Step 5: Repeat Step 1(until the number of trees we set to train is reached)\n",
    "\n",
    "- Step 6: Make the final prediction\n",
    "The AdaBoost makes a new prediction by adding up the weight (of each tree) multiply the prediction (of each tree). Obviously, the tree with higher weight will have more power of influence the final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees\n",
    "The approach trains learners based upon minimizing the loss function of a strong learner (i.e., training on the residuals of the strong model) as an alternative means to focus on training upon misclassified observations.\n",
    "\n",
    "The contribution of each weak learner to the final prediction is based on a gradient optimization process to minimize the overall error of the strong learner.\n",
    "\n",
    "Weak learners are decision trees constructed in a greedy manner with split points based on purity scores (i.e., Gini, minimize loss) thus larger trees can be used around 4 to 8 levels. Learners should still remain weak and so they should be constrained (i.e., maximum number of layers, nodes, splits, leaf nodes)\n",
    "\n",
    "Gradient boosting re-defines boosting as a numerical optimization problem where the objective is to minimize the loss function of the model by adding weak learners using a gradient-descent like procedure. As gradient boosting is based on minimizing a loss function, different types of loss functions can be used resulting in a flexible technique that can be applied to regression, multi-class classification, etc.\n",
    "\n",
    "Intuitively, gradient boosting is a stage-wise additive model that generates learners during the learning process (i.e., trees are added one at a time, and existing trees in the model are not changed). It builds a first learner to predict the observations in the training dataset, and then it calculates the loss (i.e., the value between the first learner's outcomes and the actual values). It will build a second learner that is fitted/trained on the residual error produced by the first learner to predict the loss after the first step and continue to do so until it reachest a threshold (i.e., residuals are zero). By training the second learner on the gradient of the error with respect to the loss predictions of the first learner, the second learner is being trained to correct the mistakes of the first model. This is the core of gradient boosting, and allows many simple learners to compensate for each otherâ€™s weaknesses to better fit the data.\n",
    "\n",
    "Gradient boosting does not modify the sample distribution as weak learners train on the remaining residual errors of a strong learner (i.e, pseudo-residuals). By training on the residuals of the model, this is an alternative means to give more importance to misclassified observations.\n",
    "\n",
    "Intuitively, new weak learners are being added to concentrate on the areas where the existing learners are performing poorly.\n",
    "The contribution of the weak learner to the ensemble is based on a gradient descent optimization process. The calculated contribution of each tree is based on minimizing the overall error of the strong learner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://hackernoon.com/hn-images/1*DZE1JsoE-JyxiHZzzF1owg.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://hackernoon.com/hn-images/1*DZE1JsoE-JyxiHZzzF1owg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://hackernoon.com/hn-images/1*swscIYlcOcdHjjzG0IE7dw.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://hackernoon.com/hn-images/1*swscIYlcOcdHjjzG0IE7dw.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/1868/1*tNYXUUU23kcoiww26Uh6jw.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://miro.medium.com/max/1868/1*tNYXUUU23kcoiww26Uh6jw.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "`min_samples_split`: Minimum number of observation which is required in a node to be considered for splitting. It is used to control overfitting.\n",
    "\n",
    "`min_samples_leaf` : Minimum samples required in a terminal or leaf node. Lower values should be chosen for imbalanced class problems since the \n",
    "                    regions in which the minority class will be in the majority will be very small.\n",
    "\n",
    "`min_weight_fraction_leaf` : similar to the previous but defines a fraction of the total number of observations instead of an integer.\n",
    "\n",
    "`max_depth` : maximum depth of a tree. Used to control overfitting.\n",
    "\n",
    "`max_lead_nodes` : maximum number of terminal leaves in a tree. If this is defined max_depth is ignored.\n",
    "\n",
    "`max_features` : number of features it should consider while searching for the best split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "\n",
    "- Step 1: Train a decision tree\n",
    "\n",
    "- Step 2: Apply the decision tree just trained to predict\n",
    "\n",
    "- Step 3: Calculate the residual of this decision tree, Save residual errors as the new y\n",
    "\n",
    "- Step 4: Repeat Step 1 (until the number of trees we set to train is reached)\n",
    "\n",
    "- Step 5: Make the final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Library\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Step1: Create data set\n",
    "X, y = make_moons(n_samples=10000, noise=.5, random_state=0)\n",
    "# Step2: Split the training test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier:\n",
      " Accuracy score is 0.757\n",
      " AUC score is: 0.758\n",
      "Wall time: 30.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 3: Fit a Decision Tree model as comparison\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_dt = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_dt)\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"{clf.__class__.__name__}:\\n Accuracy score is {accuracy_score(y_test, y_pred_dt):0.3f}\\n AUC score is: {roc_auc_score(y_test, y_pred_proba):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier:\n",
      " Accuracy score is 0.796\n",
      " AUC score is: 0.876\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 4: Fit a Random Forest model, \" compared to \"Decision Tree model, accuracy go up by 5%\n",
    "clf = RandomForestClassifier(n_estimators=100, max_features=\"auto\",random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"{clf.__class__.__name__}:\\n Accuracy score is {accuracy_score(y_test, y_pred):0.3f}\\n AUC score is: {roc_auc_score(y_test, y_pred_proba):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier:\n",
      " Accuracy score is 0.833\n",
      " AUC score is: 0.908\n",
      "Wall time: 595 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 5: Fit a AdaBoost model, \" compared to \"Decision Tree model, accuracy go up by 10%\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"{clf.__class__.__name__}:\\n Accuracy score is {accuracy_score(y_test, y_pred):0.3f}\\n AUC score is: {roc_auc_score(y_test, y_pred_proba):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier:\n",
      " Accuracy score is 0.834\n",
      " AUC score is: 0.907\n",
      "Wall time: 741 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 6: Fit a Gradient Boosting model, \" compared to \"Decision Tree model, accuracy go up by 10%\n",
    "clf = GradientBoostingClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"{clf.__class__.__name__}:\\n Accuracy score is {accuracy_score(y_test, y_pred):0.3f}\\n AUC score is: {roc_auc_score(y_test, y_pred_proba):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier:\n",
      " Accuracy score is 0.820\n",
      " AUC score is: 0.895\n",
      "Wall time: 301 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# XGBoost \n",
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier()\n",
    "# n_estimators = 100 (default)\n",
    "# max_depth = 3 (default)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"{clf.__class__.__name__}:\\n Accuracy score is {accuracy_score(y_test, y_pred):0.3f}\\n AUC score is: {roc_auc_score(y_test, y_pred_proba):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier:\n",
      " Accuracy score is 0.829\n",
      " AUC score is: 0.902\n",
      "Wall time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "clf = LGBMClassifier()\n",
    "# n_estimators = 100 (default)\n",
    "# max_depth = 3 (default)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"{clf.__class__.__name__}:\\n Accuracy score is {accuracy_score(y_test, y_pred):0.3f}\\n AUC score is: {roc_auc_score(y_test, y_pred_proba):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!py -3.7 -m pip install catboost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoostClassifier:\n",
      " Accuracy score is 0.831\n",
      " AUC score is: 0.906\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "clf = CatBoostClassifier(verbose=False)\n",
    "# n_estimators = 100 (default)\n",
    "# max_depth = 3 (default)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"{clf.__class__.__name__}:\\n Accuracy score is {accuracy_score(y_test, y_pred):0.3f}\\n AUC score is: {roc_auc_score(y_test, y_pred_proba):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stacking, an algorithm takes the outputs of sub-models as input and attempts to learn how to best combine the input predictions to make a better output prediction.\n",
    "\n",
    "It may be helpful to think of the stacking procedure as having two levels: level 0 and level 1.\n",
    "\n",
    "Level 0: The level 0 data is the training dataset inputs and level 0 models learn to make predictions from this data.\n",
    "Level 1: The level 1 data takes the output of the level 0 models as input and the single level 1 model, or meta-learner, learns to make predictions from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time       0.992346\n",
       "score_time     0.008184\n",
       "test_score     0.894616\n",
       "train_score    0.948999\n",
       "dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "import pandas as pd\n",
    "estimators = [\n",
    "     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "     ('svr', make_pipeline(StandardScaler(), LinearSVC(random_state=42))) ]\n",
    "\n",
    "clf = StackingClassifier( estimators=estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "scores = cross_validate(clf, X, y, cv=5, n_jobs=-1, scoring='roc_auc', return_train_score=True )\n",
    "df_score = pd.DataFrame( scores)\n",
    "df_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First we need to split data into 2 train sets and one test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=10000, noise=.5, random_state=0)\n",
    "\n",
    "X_train, X_test_f, y_train, y_test_f = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_train2, y_train, y_train2 = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We train these models with `fit(X_train, y_train)`\n",
    "- We crete a matrix out of predicted result of 3 models\n",
    "- `X_train2` is the input to initial models and `X_stack` is matrix of preditios for this data\n",
    "- `X_stack` is the predicted values for `X_train2` by initial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lgbm = LGBMClassifier().fit(X_train, y_train)\n",
    "cat= CatBoostClassifier(verbose=False).fit(X_train, y_train)\n",
    "xgb = XGBClassifier().fit(X_train, y_train)\n",
    "\n",
    "X_stack = np.hstack( (lgbm.predict(X_train2).reshape(-1, 1), cat.predict(X_train2).reshape(-1, 1), xgb.predict(X_train2).reshape(-1, 1)))\n",
    "X_stack #is calculated from X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 2), (1600, 3))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape, X_stack.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the final model with `fit(X_stack, y_train2)`\n",
    "- `X_stack` is thecombination of predicted values for `X_train2` by 3 initial models\n",
    "- `y_train2` is the actual values for `X_stack`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 1, 10], 'penalty': ['l2', 'l1']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "param_gird = {'C': [0.1, 1, 10],\n",
    "              'penalty' : ['l2', 'l1'] }\n",
    "\n",
    "grid = GridSearchCV( lr, param_gird, scoring='roc_auc', n_jobs=-1)\n",
    "grid.fit(X_stack, y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make the `X_stack_f ` for test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stack_f = np.hstack( (lgbm.predict(X_test_f).reshape(-1, 1), cat.predict(X_test_f).reshape(-1, 1), xgb.predict(X_test_f).reshape(-1, 1)))\n",
    "X_stack_f #is calculated from X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000,), (2000, 3))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_f.shape, X_stack_f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate our final model on test data\n",
    "- `X_stack_f` is the predicted values for `X_test` by initial models\n",
    "- `y_test_f` is the actual values for `X_stack_f`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8427637763776378"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_stack_f, y_test_f)\n",
    "roc_auc_score(y_test_f, grid.predict_proba(X_stack_f)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
